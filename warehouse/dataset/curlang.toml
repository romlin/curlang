# curlang.toml
# WIP ({{current_date}})

version = "0.0.1"

[environment]
model_name = "dataset"
python_version = "3.12"

[[port]]
external = 8080
internal = 80

[directories]
output = "output"

[packages]

[packages.unix]
build-essential = "*"
cmake = "*"
gcc = "*"
git = "*"
jq = "*"
python3-dev = "*"
python3-pip = "*"
sox = "*"
wget = "*"

[packages.python]
llama-cpp-python = "0.3.7 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
tiktoken = "0.9.0"

# [[git]]
# from_source = "{{git_source_url}}"
# to_destination = "{{git_destination_dir}}"
# branch = "{{branch_name}}"
# requirements_file = "requirements.txt"
# setup_commands = [
#   "{{setup_command}}"
# ]

[[file]]
from_source = "https://raw.githubusercontent.com/romlin/curlang/refs/heads/main/warehouse/dataset/curlang.json"
to_destination = "curlang.json"

[[file]]
from_source = "https://raw.githubusercontent.com/romlin/curlang/refs/heads/main/warehouse/dataset/build.sh"
to_destination = "build.sh"

[[file]]
from_source = "https://raw.githubusercontent.com/romlin/curlang/refs/heads/main/warehouse/dataset/device.sh"
to_destination = "device.sh"

[[run]]
command = "chmod +x"
file = "build.sh"

[[run]]
command = "chmod +x"
file = "device.sh"