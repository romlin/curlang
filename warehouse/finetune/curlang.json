[
  {
    "id": "part-bvyg3ryjFk",
    "type": "python",
    "disabled": false,
    "code": "import os\nimport subprocess\nimport torch\nimport logging\n\nfrom datasets import load_dataset\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments\n)\nfrom transformers import logging as hf_logging\n\nhf_logging.set_verbosity_error()\n\nos.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nmodel_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\ndataset_file = \"dataset.jsonl\"\ndataset_url = \"https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\"\ndataset_format = \"json\"\nsubset_size = 10000\n\nif not os.path.exists(dataset_file):\n    subprocess.run([\"wget\", dataset_url, \"-O\", dataset_file], check=True)\n\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nconfig = AutoConfig.from_pretrained(model_name)\nconfig.tie_word_embeddings = True\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    config=config,\n    low_cpu_mem_usage=True\n).to(device)\n\nfor param in model.parameters():\n    param.requires_grad = True\n\nmodel.config.use_cache = False\n\nif hasattr(model.config, \"loss_type\"):\n    model.config.loss_type = \"ForCausalLMLoss\"\n\nif hasattr(model.config, \"use_cache\"):\n    model.config.use_cache = False\n\nif hasattr(model, \"gradient_checkpointing_enable\"):\n    model.gradient_checkpointing_enable()\n\ndataset = load_dataset(dataset_format, data_files={\"train\": dataset_file}, split=\"train\")\n\ndef format_example(example):\n    return \" \".join([str(example[k]) for k in example if isinstance(example[k], str) and example[k].strip()])\n\ndef tokenize_example(example):\n    return tokenizer(\n        format_example(example),\n        max_length=384,\n        truncation=True\n)\n\ntokenized_dataset = dataset.map(tokenize_example, batched=False, remove_columns=dataset.column_names).filter(lambda x: len(x[\"input_ids\"]) > 0)\n\nif subset_size is not None:\n    tokenized_dataset = tokenized_dataset.shuffle(seed=42).select(range(subset_size))\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntraining_args = TrainingArguments(\n    bf16=True,\n    dataloader_num_workers=2,\n    fp16=False,\n    gradient_accumulation_steps=2,\n    learning_rate=3e-5,\n    logging_steps=10,\n    num_train_epochs=100,\n    output_dir=\"./results\",\n    per_device_train_batch_size=2,\n    remove_unused_columns=False,\n    report_to=[],\n    save_steps=100,\n    save_total_limit=1\n)\n\noptimizer = AdamW(model.parameters(), lr=3e-5)\n\nnum_training_steps = len(tokenized_dataset) // training_args.gradient_accumulation_steps * training_args.num_train_epochs\n\nlr_scheduler = LambdaLR(optimizer, lambda current_step: max(0.0, 1 - current_step / num_training_steps))\n\nif torch.cuda.is_available():\n    model = torch.compile(model)\nelif torch.backends.mps.is_available():\n    pass\n\nresume_training = True\nlast_checkpoint = None\n\nif resume_training and os.path.isdir(training_args.output_dir):\n    checkpoints = [os.path.join(training_args.output_dir, d) for d in os.listdir(training_args.output_dir) if d.startswith(\"checkpoint-\")]\n    \n    if checkpoints:\n        last_checkpoint = max(checkpoints, key=os.path.getmtime)\n        print(\"Resuming from checkpoint:\", last_checkpoint)\n\ntrainer = Trainer(\n    args=training_args,\n    data_collator=data_collator,\n    model=model,\n    optimizers=(optimizer, lr_scheduler),\n    train_dataset=tokenized_dataset\n)\n\ntrainer.train(resume_from_checkpoint=last_checkpoint)\n\nmodel.tie_weights()\n\nif torch.cuda.is_available():\n    model = torch.compile(model)\n\nmodel.save_pretrained(\"./fine-tuned-model\")\ntokenizer.save_pretrained(\"./fine-tuned-model\")"
  },
  {
    "id": "part-rPDsw1xgA3",
    "type": "python",
    "disabled": true,
    "code": "import torch\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n\nmodel_path = \"./fine-tuned-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprompt = \"Can you tell me about Paris?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\noutput = model.generate(\n    **inputs,\n    do_sample=True,\n    max_new_tokens=100,\n    pad_token_id=tokenizer.eos_token_id,\n    temperature=0.7,\n)\n\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\n\nif response.startswith(prompt):\n    response = response[len(prompt):].strip()\n\nprint(\"Prompt:\", prompt)\nprint(\"Response:\", response)"
  }
]