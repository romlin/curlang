[
  {
    "id": "part-XtQjvs7QHW",
    "type": "markdown",
    "disabled": false,
    "code": "# robot\nBuilt with Curlang"
  },
  {
    "id": "part-057pQ8XVOG",
    "type": "curlang",
    "disabled": false,
    "code": "# Download granite-3.1-2b-instruct-Q4_K_M.gguf\n!find \"granite-3.1-2b-instruct-Q4_K_M.gguf\" {\n    get \"https://huggingface.co/bartowski/granite-3.1-2b-instruct-GGUF/resolve/main/granite-3.1-2b-instruct-Q4_K_M.gguf\" as \"granite-3.1-2b-instruct-Q4_K_M.gguf\" {\n        pass \"Download completed!\"\n        fail \"Download failed!\"\n    }\n} else \"granite-3.1-2b-instruct-Q4_K_M.gguf already exists.\""
  },
  {
    "id": "part-7lha3CUpzw",
    "type": "curlang",
    "disabled": false,
    "code": "# Check whether granite-3.1-2b-instruct-Q4_K_M.gguf exists\nfind \"granite-3.1-2b-instruct-Q4_K_M.gguf\" {\n    print \"granite-3.1-2b-instruct-Q4_K_M.gguf exists.\"\n} else \"granite-3.1-2b-instruct-Q4_K_M.gguf is missing.\""
  },
  {
    "id": "part-xQhIT42IB6",
    "type": "curlang",
    "disabled": false,
    "code": "use FastAPI:fastapi, Llama:llama_cpp, os, uvicorn"
  },
  {
    "id": "part-X0nW9si3Kr",
    "type": "curlang",
    "disabled": false,
    "code": "python {\n    app = FastAPI()\n\n    llm = Llama(\n        model_path=\"granite-3.1-2b-instruct-Q4_K_M.gguf\",\n        n_ctx=131072,\n        verbose=False\n    )\n\n    ROBOT_PROMPT = \"Control a robotic arm with joints: B rotates on Y (unlimited), S on X (0째-45째), and E on X (-90째 to 45째). Each command consists of a joint letter followed by a signed angle change (for example, B+10). Ignore M commands. Generate a realistic industrial movement sequence with up to 20 commands, using purposeful, incremental moves while respecting limits. Output the result as one comma-separated line with no quotes, periods, or extra characters.\"\n\n    @app.post(\"/generate\")\n    def generate_robot_commands():\n        input_text = (\n            f\"<|start_of_role|>system<|end_of_role|>You are a helpful assistant. Be concise and direct.<|end_of_text|>\"\n            f\"<|start_of_role|>user<|end_of_role|>{ROBOT_PROMPT}<|end_of_text|><|start_of_role|>assistant<|end_of_role|>\"\n        )\n        response = llm(\n            input_text,\n            max_tokens=512,\n            stop=[\"<|endoftext|>\"],\n            seed=-1,\n            temperature=0.7\n        )\n        output = response[\"choices\"][0][\"text\"].strip()\n        \n        with open(\"output/output.txt\", \"w\") as f:\n            f.write(output)\n        \n        result_dict = {\"result\": output}\n        return result_dict\n    \n    uvicorn.run(app, host=\"127.0.0.1\", port=4000)\n}"
  }
]